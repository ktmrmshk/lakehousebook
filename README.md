# Understanding Lakehouse in practice - from CSV to レイクハウス

## はじめに

この本を手に取った方であれば、仕事や趣味などで「データを扱う」ということを何かしらされている場合が多いと思います。いやいや、そもそも一般に生きていれば「データ」は溢れているし、「データ」に触れないで、意識しているかしていないかを除けば、完全に「データ」から逃れて生活することは難しいと思います。例えば、出かけるときに服装や持ち物をどうしようかと「天気予報」を見て決めるし、晴れの天気予報だけど最近は朝晴れている場合、職場の周りは夕立が多く、折り畳み傘を持って行った方がいい、などというわけです。ここでは、「天気予報」に加えて、今までの経験値が「データ」として予測に使われているわけです。

ということで、「データを扱う」というのは、もはや壮大なテーマであり、一概にこういう方法やツールで完全に対応できるという話ではないわけです。ではどうするかというと、「データを扱う」人が適切に考えて、管理し、操作することが唯一の解なのかなと思います。その意味で、ここでは、特に現在のITシステムやAI(人工知能)周りで問題になりやすい3Vな「データ」を扱うプラクティスを実際に体感しながら、「データを扱う上での勘所が感覚として解る」というのを目標に話を進めていきたいと思います。

私も含め、一般の人間は最初に頭で考えるよりも、まずは、体感して、感覚的に身につけ、それから頭を使って汎化し、一般化知識にしていく、というのがうまくいくことが多いと思い気がします。その立場で、テキスト上の数値データであるCSVファイルから初めて、昨今新しいデータアーキテクチャである「レイクハウス」を理解するところまでを見ていきます。

「レイクハウス」はデータを効率的・効果的に扱うためのプラクティスから生まれた「考え方」であり、何かのツールやベンダ固有の製品・サービスではありません。そのため、ここで目標にしている「体感的にわかる」状態になれば、皆さんの好きなツールやサービスで実現・構成することが可能です。ただし、具体例がないと「体感的に」分かりづらい部分がありますので、今回の説明では、以下のオープンソースのフレームワークを使用します。プレーンなLinux環境から始めて、実際のコードも載せますので、実行しながら体感してください。

* 計算機(+物理ストレージ): Linux (Ubuntu) (+ローカルディスク)
* 処理エンジン: Apache Spark
* ストレージエンジン: Deltalake

それでは、早速いきましょう。

## データを扱う

漠然としてますが、皆さんは「データ」といったら何を思い浮かべますか?色々な「データ」があると思います。例えば、「過去１年間分の天気予報」、「電車の運行ダイヤ」、「スマホで取ってきた写真」、「ビデオや音楽」、「ブログ記事」、「株価」、「円周率」、「Webサイトのアクセスログ」、「レシート・購入履歴」など、いろいろあると思います。これらのデータを分類すると大きく2つに分類できると思います。

* **構造化データ** データの形式が完全に固定されているもの
* **非構造化データ** 構造化データではないもの

例えば、「過去１年間分の天気予報」は、前者の構造化データになりと思います。天気予報は、おそらく、日時、場所、天気(晴れ、雨など)などが含まれるでしょう。これらのデータは固定的に扱うことができます。日時であれば、「2023-04-01」と形式化できますし、場所についても、緯度軽度の数値(小数点)、天気は固定的な文字列の中から選ばれるようになります。このように、データの形式が固定的になるので、構造化データとして捉えることが可能です。

一方で、「ビデオや音楽」はどうでしょうか? 一言で言いづらいかもしれませんが、写真は画像データであり、ファイルフォーマット、解像度、サイズなどがバラバラである可能性が高いですね。データの形式が固定的になりずらいため、非構造化データになると思います。

それでは、それ以外に例で挙げたデータがどちらになるか、考えてみてください。私は以下のように分類しまいた。

* 構造化データ
** 電車の運行ダイヤ (列車の識別番号、駅名、到着時刻、出発時刻)
** 株価 (日付、銘柄、株価)
** 円周率 (数値)
** 購入履歴 (日時、商品、個数、単価、支払い金額)
** webサイトのアクセスログ (日時、アクセス元IP、ページ、レスポンスコード、エラーメッセージ)

* 非構造化データ
** ブログ記事(タイトル、本文、写真などが混ざったドキュメント)
** レシート(購入記録などがカスタムのレイアウトで表現される)

ここで、「おやっ!?」と思った方もいるかと思います。例えば、ブログ記事については、タイトル(文字列)、本文(文字列)、画像(バイナリ形式)とすれば構造化データと見えるし、実際多くのブログサイトシステム(CMS)では構造化データを扱うRDBMS(MySQLやPostgresSQLなど)にデータを蓄積する構成を取っていたりします。

そうです。構造化データと非構造化データの区切りは、実際には曖昧になっています。データをシステムで扱う上では、「テーブル」構成になっているかが一つの区切りになっています。構造化データは、テーブル構造をとることができ、かつ、その属性(カラム、列)が固定されたデータ形式で定義できるものを指します。

ブログ記事を考えると、ブログサイト全体として、タイトル(文字列)、本文(文字列)、添付データ(バイナリ)という3つのカラムを用意して管理されていれば、それは構造化データといえます。逆に、ウェブ上のブログサイトからスクレイピングしてきたような状態は、単に整理されていない文字列(例えばHTMLやDOMデータ)であるので、非構造データになります。

特に、構造化データは管理して意味がある状態のテーブルであり、一般的には、飛行増加データを整理していくと構造化データになっていきます。

例えば、webアクセスログなども、通常は、web serverから測れた状態では、単なる文字列ですが、そこからパースして、意味があるテーブルデータにしていくことが一般的な流れになっています。

## テーブルデータとCSV



## データベース
## データレイク
## スキーマ on read
## データ更新とデータベース
## データ更新とストレージレイヤ
## ストレージとコンピュートの分離
## レイクハウスとDelta Lake
## ガバナンスとメタデータ管理