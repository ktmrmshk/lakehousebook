# Understanding Lakehouse in practice - from CSV to レイクハウス

## はじめに

この本を手に取った方であれば、仕事や趣味などで「データを扱う」ということを何かしらされている場合が多いと思います。いやいや、そもそも一般に生きていれば「データ」は溢れているし、「データ」に触れないで、意識しているかしていないかを除けば、完全に「データ」から逃れて生活することは難しいと思います。例えば、出かけるときに服装や持ち物をどうしようかと「天気予報」を見て決めるし、晴れの天気予報だけど最近は朝晴れている場合、職場の周りは夕立が多く、折り畳み傘を持って行った方がいい、などというわけです。ここでは、「天気予報」に加えて、今までの経験値が「データ」として予測に使われているわけです。

ということで、「データを扱う」というのは、もはや壮大なテーマであり、一概にこういう方法やツールで完全に対応できるという話ではないわけです。ではどうするかというと、「データを扱う」人が適切に考えて、管理し、操作することが唯一の解なのかなと思います。その意味で、ここでは、特に現在のITシステムやAI(人工知能)周りで問題になりやすい3Vな「データ」を扱うプラクティスを実際に体感しながら、「データを扱う上での勘所が感覚として解る」というのを目標に話を進めていきたいと思います。

私も含め、一般の人間は最初に頭で考えるよりも、まずは、体感して、感覚的に身につけ、それから頭を使って汎化し、一般化知識にしていく、というのがうまくいくことが多いと思い気がします。その立場で、テキスト上の数値データであるCSVファイルから初めて、昨今新しいデータアーキテクチャである「レイクハウス」を理解するところまでを見ていきます。

「レイクハウス」はデータを効率的・効果的に扱うためのプラクティスから生まれた「考え方」であり、何かのツールやベンダ固有の製品・サービスではありません。そのため、ここで目標にしている「体感的にわかる」状態になれば、皆さんの好きなツールやサービスで実現・構成することが可能です。ただし、具体例がないと「体感的に」分かりづらい部分がありますので、今回の説明では、以下のオープンソースのフレームワークを使用します。プレーンなLinux環境から始めて、実際のコードも載せますので、実行しながら体感してください。

* 計算機(+物理ストレージ): Linux (Ubuntu) (+ローカルディスク)
* 処理エンジン: Apache Spark
* ストレージエンジン: Deltalake

それでは、早速いきましょう。

## データを扱う

漠然としてますが、皆さんは「データ」といったら何を思い浮かべますか?色々な「データ」があると思います。例えば、「過去１年間分の天気予報」、「電車の運行ダイヤ」、「スマホで取ってきた写真」、「ビデオや音楽」、「ブログ記事」、「株価」、「円周率」、「Webサイトのアクセスログ」、「レシート・購入履歴」など、いろいろあると思います。これらのデータを分類すると大きく2つに分類できると思います。

* **構造化データ** データの形式が完全に固定されているもの
* **非構造化データ** 構造化データではないもの

例えば、「過去１年間分の天気予報」は、前者の構造化データになりと思います。天気予報は、おそらく、日時、場所、天気(晴れ、雨など)などが含まれるでしょう。これらのデータは固定的に扱うことができます。日時であれば、「2023-04-01」と形式化できますし、場所についても、緯度軽度の数値(小数点)、天気は固定的な文字列の中から選ばれるようになります。このように、データの形式が固定的になるので、構造化データとして捉えることが可能です。

一方で、「ビデオや音楽」はどうでしょうか? 一言で言いづらいかもしれませんが、写真は画像データであり、ファイルフォーマット、解像度、サイズなどがバラバラである可能性が高いですね。データの形式が固定的になりずらいため、非構造化データになると思います。

それでは、それ以外に例で挙げたデータがどちらになるか、考えてみてください。私は以下のように分類しまいた。

* 構造化データ
  * 電車の運行ダイヤ (列車の識別番号、駅名、到着時刻、出発時刻)
  * 株価 (日付、銘柄、株価)
  * 円周率 (数値)
  * 購入履歴 (日時、商品、個数、単価、支払い金額)
  * webサイトのアクセスログ (日時、アクセス元IP、ページ、レスポンスコード、エラーメッセージ)

* 非構造化データ
  * ブログ記事(タイトル、本文、写真などが混ざったドキュメント)
  * レシート(購入記録などがカスタムのレイアウトで表現される)

ここで、「おやっ!?」と思った方もいるかと思います。例えば、ブログ記事については、タイトル(文字列)、本文(文字列)、画像(バイナリ形式)とすれば構造化データと見えるし、実際多くのブログサイトシステム(CMS)では構造化データを扱うRDBMS(MySQLやPostgresSQLなど)にデータを蓄積する構成を取っていたりします。

そうです。構造化データと非構造化データの区切りは、実際には曖昧になっています。データをシステムで扱う上では、「テーブル」構成になっているかが一つの区切りになっています。構造化データは、テーブル構造をとることができ、かつ、その属性(カラム、列)が固定されたデータ形式で定義できるものを指します。

ブログ記事を考えると、ブログサイト全体として、タイトル(文字列)、本文(文字列)、添付データ(バイナリ)という3つのカラムを用意して管理されていれば、それは構造化データといえます。逆に、ウェブ上のブログサイトからスクレイピングしてきたような状態は、単に整理されていない文字列(例えばHTMLやDOMデータ)であるので、非構造データになります。

特に、構造化データは管理して意味がある状態のテーブルであり、一般的には、飛行増加データを整理していくと構造化データになっていきます。

例えば、webアクセスログなども、通常は、web serverから測れた状態では、単なる文字列ですが、そこからパースして、意味があるテーブルデータにしていくことが一般的な流れになっています。

-- 時間があれば、web logのパースを見ていく。

## テーブルデータとCSV

ここからは構造化データ、つまりテーブルデータを見ていきます。みなさんがテーブルデータを扱う際に使用するツールやファイルフォーマットはどんなものがありますか?一番メジャー(?)なツールは表計算ソフト(ExcelやSpreadsheetなど)で、ファイルフォーマットはCSV(コンマ区切りでデータが並んでいる文字列ファイル)だと思います。

実は、みなさんもご存知の通り、他にも多くのツールやファイルフォーマットが存在します。
それでは、なぜ、CSVファイルが多く使われるのでしょうか?
理由はいくつかあると思いますが、端的にいうと「多くの人にとって使いやすい、わかりやすい」からというのがあると思います。CSVは、テキストエディタでも編集できるし、各種ツールでもimport/exportがサポートされている現実もあると思います。

例えば、ブログサイトのエントリーを管理しているCSVファイルを見てみましょう。

File: `my_blog.csv`
```csv
#タイトル,カテゴリ,評価,公開日時
山手線一周,旅行,3.4,2023-04-01
データ分析入門,分析,4.3,2023-03-02
昨日のご飯,生活,3,2023-02-14
```

テキストファイルなので、見やすく、編集もしやすいですね。
テーブルデータを扱うには、CSV一択でいい気もします。しかし、皆さんもお気づきのように、CSVファイルには多くの弱点があります。以下にいくつか挙げてみます。

1. 編集ミスなどで、データが壊れやすくなる
1. レコード数、カラム数が膨大になると、もはや可読性が低くなる
2. 文字列なので、各カラムのデータ型に厳密性がない
3. テキストファイルなので、圧縮されておらず、ファイルサイズが大きくなる
4. (レコード数が大規模になると)集計処理が遅い
5. 複数人で同時に編集した際に、データが壊れる可能性がある
6. いつ、誰が、どの部分のデータを改変したのかがわからない

一人で趣味程度に管理しているデータであれば問題ないかもしれませんが、業務や本番システムで使うには、非常に心許無いのです。これらを解決するのが、データベースシステムなのです。


## データベースシステム

データベースは、先ほど挙げたCSVファイルの弱点を全て克服する性能・機能を持っています。
ここでは主にテーブルデータを扱うのに特化したリレーショナルデータベース(Relational Data Base Management System; RDBMS)を想定します。MySQLやSQLiteなどをイメージしていただければ問題ありません。

これらのデータベースでは、データをテーブルとして使用する際に、カラムとそのデータ型(これをスキーマと呼びます)を厳密に定義して、その後データを挿入(投入)していくスタイルを取ります。先にデータ型を定義しておくことで、汚れたデータを入り口で弾くようなフィルターとして機能させることができるようになっています。「日時型」のカラムに数値を入れようとするとエラーで弾かれます。このように、データを書く前にスキーマを定義しておく方式をスキーマ・オン・ライト(Schema on write)と呼びます。

それでは、SQLiteで先ほどのCSVで表現したテーブルを作成したいと思います。
SQLiteはローカルで軽量に動く、サーバが不要、Pythonに標準で組み込まれている、データタイプが5種類しかなく極めてシンプル、という理由から、個人的に結構好きなデータベースです。

```sql
CREATE TABLE my_blog (
    `タイトル` TEXT,
    `カテゴリ` TEXT,
    `評価` REAL,
    `公開日時`　DATE
);

INSERT INTO my_blog VALUES
("山手線一周","旅行", 3.4, "2023-04-01"),
("データ分析入門","分析", 4.3, "2023-03-02"),
("昨日のご飯","生活", 3, "2023-02-14");
```

最後に、上記で作成したテーブルを表示させてみましょう。

```sql
.mode column -- 表示モードを整える
.headers on -- ヘッダーを表示される

SELECT * FROM my_blog;

タイトル     カテゴリ  評価   公開日時      
-------  ----  ---  ----------
山手線一周    旅行    3.4  2023-04-01
データ分析入門  分析    4.3  2023-03-02
昨日のご飯    生活    3.0  2023-02-14
```

このようにデータをきっちり収める場合には、確かにこちらの方が適しているようにも思えます。
しかし、実際に運用してみると、文脈によっては、意外にこの方式だとメンドーなこともあったりします。例えば、データ型は意外に運用しているうちに変更されることがあったり、カラムが追加になったり、データベースに入れるデータ量が大きすぎて`INSERT`処理に時間がかかったりと。

また、構造化される前にデータや、そもそも画像データのように非構造なデータも一緒に扱いたい場合も出てきます。こうした、そこまで「きっちり」しなくてもいい、むしろ、データベースのような厳密性は若干かけるけど、柔軟にデータを管理したい場合には、「データ」をそのままファイルとして扱っておいて、そのデータが必要になったタイミングで、構造化処理を実施すれば良いのではないか、という考えに行き着きます。これがデータレイクの考え方です。

## データレイクとスキーマ・オン・リード

データレイクは、先ほど書いたように、データを「テーブル」ではなく、「ファイル」として置いておくための保存場所です。データレイクが登場した背景は、データベースが持つスキーマ・オン・ライト方式の課題解決に加えて、ストレージが安価になり、大規模なファイルを保存して置いても、そこまで多くのお金がかからなくなった、という理由もあります。主に使用されるのは、オブジェクトストレージ(AWS S3やAzure Blogストレージなど)です。



## データ更新とデータベース
## データ更新とストレージレイヤ
## ストレージとコンピュートの分離
## レイクハウスとDelta Lake
## ガバナンスとメタデータ管理